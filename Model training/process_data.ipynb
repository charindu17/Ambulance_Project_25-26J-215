{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45da4a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 files: ['10min- 1.csv', '10min- 2.csv', '10min- 3.csv', '13min.csv', '18min- 1.csv', '18min- 2.csv']\n",
      "Processing 10min- 1.csv...\n",
      "Processing 10min- 2.csv...\n",
      "Processing 10min- 3.csv...\n",
      "Processing 13min.csv...\n",
      "Processing 18min- 1.csv...\n",
      "Processing 18min- 2.csv...\n",
      "Generated 6623 training samples.\n",
      "Saved to 'training_data.json'. READY FOR AI!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # converts the JSON into a DataFrame.\n",
    "import numpy as np # used internally by pandas & sklearn.\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# CONFIGURATION\n",
    "WINDOW_SIZE = 50   # 50 readings = approx 1 second (at 20ms rate)\n",
    "STEP_SIZE = 10     # Overlap windows to get more data\n",
    "INPUT_FILES = \"*.csv\" # Finds all your CSVs\n",
    "\n",
    "def calculate_magnitude(row): #function that takes one sensor data row (main preprocessing step)\n",
    "\n",
    "    return np.sqrt(row['acc_x']**2 + row['acc_y']**2 + row['acc_z']**2) #Turns 3-axis accelerometer readings into a single number called magnitude/total force.\n",
    "\n",
    "def process_files(): # the begining of the windowing + feature extraction logic \n",
    "    all_files = glob.glob(INPUT_FILES) # Finds CSV files (returns all filenames matching \"*.csv\")\n",
    "    print(f\"Found {len(all_files)} files: {all_files}\")\n",
    "\n",
    "    processed_samples = [] # Each training sample (one window of data) will be stored as a dictionary inside this list.\n",
    "    \n",
    "    for filename in all_files: #runs for no of files\n",
    "        print(f\"Processing {filename}...\") # Good for progress tracking.\n",
    "        try: \n",
    "            df = pd.read_csv(filename) # Reads the CSV file into a pandas DataFrame.\n",
    "            \n",
    "            # 1. Feature Engineering section- Now the core preprocessing starts.\n",
    "            # Calculate Total Force (Magnitude) and For every row, apply the previously defined calculate_magnitude() function\n",
    "            df['mag'] = df.apply(calculate_magnitude, axis=1) # axis=1 means \"apply function on each row\"; Converts 3 messy values into 1 clean value\n",
    "            \n",
    "            # Calculate Jerk (Change in Force)--> calculates difference between each two consecutive magnitude values.\n",
    "            df['jerk'] = df['mag'].diff().fillna(0) # fillna(0) replaces the first NaN with 0.\n",
    "            \n",
    "            # Fill missing GPS speeds with 0 to avoid errors during mean() calculations\n",
    "            df['gps_speed'] = df['gps_speed'].fillna(0) #Summarizes speed behavior over time\n",
    "\n",
    "            # 2. Sliding Window Logic\n",
    "            \n",
    "            for i in range(0, len(df) - WINDOW_SIZE, STEP_SIZE): # creates overlapping windows of sensor data. WINDOW_SIZE= training sample, STEP_SIZE= no of forward rows\n",
    "                window = df.iloc[i : i + WINDOW_SIZE] # Extract rows ex:  from index i to i+50\n",
    "                \n",
    "                # The label for this window is the most common label in these 50 rows\n",
    "                label = window['label'].mode()[0] # Mode gives the dominant driving state; cleaning; removes noise\n",
    "                \n",
    "                # Feature Extraction Block- Extract Features from this 1-second chunk\n",
    "                features = {\n",
    "                    \"label\": label,\n",
    "                    \"avg_speed\": float(window['gps_speed'].mean()), # average of the speed values in the window (typical speed)\n",
    "                    \"max_force\": float(window['mag'].max()), # Highest acceleration magnitude in the window to capture strong events (braking, sudden turns)\n",
    "                    \"std_force\": float(window['mag'].std()), # How much it vibrated--> Measures how much shaking or variation is happening.\n",
    "                    \"min_force\": float(window['mag'].min()), # Lowest magnitude to detect moments when car is steady or stopped.\n",
    "                    \"avg_jerk\": float(window['jerk'].abs().mean()) # Represents smooth vs sudden movement transitions.\n",
    "                }\n",
    "                \n",
    "                processed_samples.append(features) # Adds the computed feature dictionary to the list. ex: {\"label\":\"Cruising\", \"avg_speed\":3.2, ...}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {filename} due to error: {e}\")\n",
    "\n",
    "    # 3. Save to JSON--> the input dataset used for model training.\n",
    "    print(f\"Generated {len(processed_samples)} training samples.\")\n",
    "    with open('training_data.json', 'w') as f: # Opens a file named training_data.json in write mode.\n",
    "        json.dump(processed_samples, f) # Writes the list as formatted JSON; No NaN allowed-cleaning\n",
    "    print(\"Saved to 'training_data.json'. READY FOR AI!\")\n",
    "\n",
    "if __name__ == \"__main__\": # fill here\n",
    "    process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea51797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training_data.json...\n",
      "\n",
      "Total Samples: 6623\n",
      "Class Distribution (Before Balancing):\n",
      "label\n",
      "Cruising      4772\n",
      "Braking       1152\n",
      "Lane Left      419\n",
      "Lane Right     251\n",
      "Pullover        29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training Random Forest Model...\n",
      "\n",
      "--- MODEL REPORT CARD ---\n",
      "Overall Accuracy: 87.92%\n",
      "\n",
      "Detailed Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Braking       0.85      0.78      0.81       219\n",
      "    Cruising       0.89      0.98      0.93       952\n",
      "   Lane Left       0.86      0.56      0.68        91\n",
      "  Lane Right       0.85      0.19      0.31        59\n",
      "    Pullover       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.88      1325\n",
      "   macro avg       0.89      0.55      0.63      1325\n",
      "weighted avg       0.88      0.88      0.86      1325\n",
      "\n",
      "\n",
      "Confusion Matrix (Rows=Actual, Cols=Predicted):\n",
      "[[171  43   4   1   0]\n",
      " [ 17 931   3   1   0]\n",
      " [ 12  28  51   0   0]\n",
      " [  1  47   0  11   0]\n",
      " [  0   2   1   0   1]]\n",
      "\n",
      "--- SENSOR IMPORTANCE ---\n",
      "1. avg_speed: 0.3311\n",
      "2. min_force: 0.1949\n",
      "3. avg_jerk: 0.1877\n",
      "4. max_force: 0.1463\n",
      "5. std_force: 0.1400\n",
      "\n",
      "[SUCCESS] Model saved as 'ambulance_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # loads JSON, CSV, and manage data tables as well.\n",
    "import json\n",
    "import seaborn as sns # Makes statistical plots\n",
    "import matplotlib.pyplot as plt # creates graphs/plots\n",
    "from sklearn.model_selection import train_test_split # Splits dataset into training (80%) and testing (20%). (sklearn.model_selection- ML library used for training RandomForest)\n",
    "from sklearn.ensemble import RandomForestClassifier # Loads the RandomForest algorithm.\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score # Provides evaluation metrics.\n",
    "import joblib # save and load trained ML (.pkl file) models to disk\n",
    "# ML Model Training Section\n",
    "def train_ambulance_ai(): # performs the entire training procedure.\n",
    "    # 1. Load the processed data\n",
    "    print(\"Loading training_data.json...\")\n",
    "    try:\n",
    "        with open('training_data.json', 'r') as f: # Opens the JSON file created earlier.\n",
    "            data = json.load(f) # Loads all training samples into the variable (data).\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'training_data.json' not found. Run process_data.py first!\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data) # converts my JSON training samples into a structured Excel-like table (ML friendly)\n",
    "\n",
    "    # 2. Sanity Check\n",
    "    print(f\"\\nTotal Samples: {len(df)}\") # Checks dataset size.\n",
    "    print(\"Class Distribution (Before Balancing):\")\n",
    "    print(df['label'].value_counts()) # Shows how many samples each class (label) has.\n",
    "\n",
    "    # 3. Prepare Inputs (X) and Outputs (y)\n",
    "    X = df.drop(columns=['label']) # numeric feature columns (avg_speed, max_force..) only.\n",
    "    y = df['label'] # correct class for each window (row).\n",
    "\n",
    "    # 4. Split into Training (80%) and Testing (20%)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # ensures reproducible results by fixing the random seed. Start your random number generator from this exact position (42).\n",
    "\n",
    "    # 5. Train the Random Forest\n",
    "    # 'class_weight=\"balanced\"' is CRITICAL here. \n",
    "    # It tells the AI: \"Pay 10x more attention to 'Braking' than 'Cruising' because it's rare.\"\n",
    "    print(\"\\nTraining Random Forest Model...\")\n",
    "    model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42) # Use 100 decision trees inside the forest, helps the model correctly identify and doesn't ignore rare classes behaviors like braking, turning, or pullover.\n",
    "    model.fit(X_train, y_train) # Train the model; The model learns patterns in the training data.\n",
    "\n",
    "    # 6. Evaluate\n",
    "    print(\"\\n--- MODEL REPORT CARD ---\")\n",
    "    y_pred = model.predict(X_test) # Predict test labels; Run the trained model on the test dataset & Compare predicted labels with true labels.\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred) # Evaluate Model Performance\n",
    "    print(f\"Overall Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    print(\"\\nDetailed Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # 7. Confusion Matrix (Where did it get confused?)\n",
    "    print(\"\\nConfusion Matrix (Rows=Actual, Cols=Predicted):\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    # 8. Feature Importance (What sensors mattered?)\n",
    "    print(\"\\n--- SENSOR IMPORTANCE ---\")\n",
    "    importances = model.feature_importances_ #Shows how useful that feature was in making correct predictions\n",
    "    features = X.columns #matches each importance value to its correct name.\n",
    "    # Sort them\n",
    "    indices = importances.argsort()[::-1] #sorts the features by importance — from most important → least important. .argsort() sorts from smallest to largest.\n",
    "    for i in range(len(features)):\n",
    "        print(f\"{i+1}. {features[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "\n",
    "    # 9. Save the Model\n",
    "    joblib.dump(model, 'ambulance_model.pkl')\n",
    "    print(\"\\n[SUCCESS] Model saved as 'ambulance_model.pkl'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_ambulance_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d747fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ambulance_model.pkl...\n",
      "Converting to JavaScript...\n",
      "Success! Model saved as ambulance_model.js\n",
      "Action Required: Move 'ambulance_model.js' into your 'app' folder manually.\n"
     ]
    }
   ],
   "source": [
    "import joblib #load that saved model (.pkl) back into memory\n",
    "import m2cgen as m2c #Convert a trained machine learning model into js\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. Load trained model\n",
    "print(\"Loading ambulance_model.pkl...\")\n",
    "try:\n",
    "    model = joblib.load('ambulance_model.pkl')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ambulance_model.pkl' not found. Make sure you ran train_model.py!\")\n",
    "    exit()\n",
    "\n",
    "# 2. Convert it to JavaScript code\n",
    "print(\"Converting to JavaScript...\")\n",
    "# This turns the random forest trees into a huge \"if/else\" function\n",
    "js_code = m2c.export_to_javascript(model)\n",
    "\n",
    "# 3. Add the \"export\" keyword so React Native can use it\n",
    "final_js = \"export default \" + js_code\n",
    "\n",
    "# 4. Save it to the CURRENT directory first (safest for Jupyter)\n",
    "output_filename = 'ambulance_model.js' \n",
    "\n",
    "with open(output_filename, 'w') as f:\n",
    "    f.write(final_js)\n",
    "\n",
    "print(f\"Success! Model saved as {output_filename}\")\n",
    "print(f\"Action Required: Move '{output_filename}' into your 'app' folder manually.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
