{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3c498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "image_dataset_from_directory() got an unexpected keyword argument 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# --- 3. Load ONLY the Eye Data ---\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# We use tf.keras.utils.image_dataset_from_directory. It's smart and efficient.\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# We explicitly tell it to ONLY look for 'Open_Eyes' and 'Closed_Eyes'\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# It will automatically assign 'Open_Eyes' as 0 and 'Closed_Eyes' as 1.\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading training data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m train_eye_dataset = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbinary\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Two classes: 'Open_Eyes' (0) or 'Closed_Eyes' (1)\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOpen_Eyes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mClosed_Eyes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# This is the key! We ignore the 'Yawn' folders.\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m123\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# For reproducible results\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading validation data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m val_eye_dataset = tf.keras.utils.image_dataset_from_directory(\n\u001b[32m     36\u001b[39m     val_dir,\n\u001b[32m     37\u001b[39m     label_mode=\u001b[33m'\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     seed=\u001b[32m123\u001b[39m\n\u001b[32m     42\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: image_dataset_from_directory() got an unexpected keyword argument 'classes'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Data Paths \n",
    "base_dir = r\"C:\\Users\\janak\\OneDrive\\Desktop\\Projects\\Ambulence_Project-Charindu\\Ambulence_Project-Charindu\\archive\" \n",
    "\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "val_dir = os.path.join(base_dir, 'Val')\n",
    "\n",
    "#  Set Up Model Parameters\n",
    "IMG_SIZE = (224, 224) # MobileNetV2 was trained on 224x224 images\n",
    "BATCH_SIZE = 32       # How many images to process at once\n",
    "\n",
    "# Load the Eye Data\n",
    "# automatically find 'Open_Eyes' and 'Closed_Eyes'\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_eye_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    label_mode='binary',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=123 \n",
    ")\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "val_eye_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    label_mode='binary',\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Verify the Classes\n",
    "class_names = train_eye_dataset.class_names\n",
    "print(\"\\nSuccessfully loaded classes for Eye-State Detector:\")\n",
    "print(class_names) \n",
    "\n",
    "#  Optimize Data Loading \n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_eye_dataset = train_eye_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "val_eye_dataset = val_eye_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"\\n Data loading setup is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Create a Data Augmentation Layer \n",
    "data_augmentation = models.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#  Load the Pre-trained Base Model (MobileNetV2)\n",
    "\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=IMG_SIZE + (3,), \n",
    "    include_top=False, \n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "#  Freeze the Base Model \n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add Own Classification \"Head\"\n",
    "\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=IMG_SIZE + (3,))\n",
    "\n",
    "# Apply data augmentation\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# Pre-process the inputs (scales pixel values from [0, 255] to [-1, 1])\n",
    "# This is how MobileNetV2 expects its data.\n",
    "x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "\n",
    "# Pass the data to the frozen base_model\n",
    "x = base_model(x, training=False) # 'training=False' is important here\n",
    "\n",
    "# flatten the output from the base model\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a \"dropout\" layer to prevent overfitting\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# Add final output layer.\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create and Compile the Final Model\n",
    "\n",
    "eye_model = models.Model(inputs, outputs)\n",
    "\n",
    "# Now we compile the model\n",
    "eye_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'] \n",
    ")\n",
    "\n",
    "#  Print a Summary\n",
    "\n",
    "eye_model.summary()\n",
    "\n",
    "print(\"\\n Eye-State Detector model is built, compiled, and ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Training Parameters \n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "#  Train the Model \n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "\n",
    "history = eye_model.fit(\n",
    "    train_eye_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_eye_dataset\n",
    ")\n",
    "\n",
    "print(\"\\n Training complete!\")\n",
    "\n",
    "#  Plot the Results (Accuracy and Loss)\n",
    "\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Training & Validation Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
